 [9,] -0.28415974  0.58318765  0.53652372 -2.12354988  1.76431407  0.09710422  0.54356763  0.97208502 -0.72732815  0.67261453
[10,] -0.91932200 -1.30679883  0.82487007 -1.19603152 -0.02367989 -0.07673369  0.75286121  0.75586993  0.98655595 -0.27751799
[11,] -0.11624781 -0.54038607 -0.96390148  1.64219199  0.19992048  0.99195068 -0.80867413 -0.42828562  1.42398360 -0.14602639
[12,]  1.81731204  1.94769266 -0.85508251  0.88365483  1.34719278 -0.85925076  1.00111985 -0.71392476  0.48473051  1.70143183
[13,]  0.37062786  0.05359027  1.88694694  0.52487589  0.03607349 -0.28157969  0.45605250 -0.19038407  0.34923645  0.47135788
[14,]  0.52021646  0.35166284 -0.39181937 -1.18465907  0.82458113  2.06624727 -1.43425031  0.39986481  0.86012425  0.58170008
[15,] -0.75053199 -0.67097654 -0.98063295  2.65578827 -1.70267185 -0.61155298 -0.26530482 -0.97784491  0.40461114  0.66602070
[16,]  0.81689984  0.27795369  0.68733210 -1.04791371  0.48095016  0.31561282  0.64176916  0.18373691  0.36704492 -0.77890223
[17,] -0.88635752  0.69117127 -0.50504352 -1.01112252  2.48355009  0.66029338 -0.41502103 -2.15031053 -1.51919905  1.16332485
[18,] -0.33157759  0.82379533  2.15771982  0.66892165  0.40136499 -1.72220241 -0.45957568 -0.62296653  1.54980333 -1.96454420
[19,]  1.12071265  2.14506502 -0.59979756  0.12917729  0.21517717 -2.13462605 -0.79249402 -0.76543932  0.49988107  0.76917067
[20,]  0.29872370 -2.34694398 -0.69454669 -0.42257687 -1.81571235  0.06894560 -1.15853913  0.46430942  0.46087285  2.25977199
[21,]  0.77962192  0.14959198  0.22392541 -1.14026414 -0.91173942  0.86782174  0.71089000  0.52228217  2.07670993 -0.47727229
[22,]  1.45578508 -1.34253148 -1.15622333 -1.29371529 -0.04904469 -2.29004418  1.26760175  0.00979376 -0.30765057 -0.10258051
[23,] -0.64432843  0.55330308  0.42241853 -0.59469877 -0.40538748 -0.15019029 -0.14315106 -0.44052620  0.95237089  0.36869618
[24,] -1.55313741  1.58996284 -1.32475526 -1.50081408  1.13038180 -0.26878179 -0.51502891  1.19948953  0.53278829 -0.53543299
[25,] -1.59770952 -0.58687959  0.14108431  0.01585569  0.81546474  1.79133204  1.48289118 -0.11746849 -0.09554771  0.50660194
[26,]  1.80509752 -1.83237731 -0.53604800  0.54016957  0.07641752  0.67226804 -0.16258891  0.03820979 -0.14206338 -0.15055710
[27,] -0.48164736  0.88813943 -0.31160608 -1.54729197  1.45374735 -0.20930114  0.04170917  1.19480563 -1.18302571  0.90424356
[28,]  0.62037980  1.59348847  1.55610964  0.84965293  0.37412108  0.01218251  0.48303990  0.34395835  0.54531841  2.24203614
[29,]  0.61212349  0.51685467 -0.44803329  0.89601318 -0.17090406  1.53411686 -1.18012717 -0.32907297 -2.58185544 -1.19512289
[30,] -0.16231098 -1.29567168  0.32112354  0.13869100 -0.50221281  0.07729182 -0.66357374  1.67085792  0.77890003 -0.41852257
[31,]  0.81187318  0.05461558 -1.23017225 -1.61932832  0.54352211  0.07843753 -0.63464989 -0.91805820  0.29294030  0.79825139
[32,]  2.19683355 -0.78464937 -1.32405869  0.54839792 -0.50518600 -0.77926107 -0.70196303 -0.08780733 -0.08670791  0.49817216
[33,]  2.04919034 -1.04935282  1.26124227  0.19528215  0.78679579  0.16655967  0.57685038  1.32029372 -1.46635950  0.11956022
[34,]  1.63244564  2.33051196  1.31923172 -0.80649799  0.30094940  0.26532457 -2.11308037  1.73078613 -1.08317945 -0.36720272
[35,]  0.25427119  1.40270538 -0.08075376 -0.10862424  1.31022391  0.89078071  0.26090968  2.16259608  1.05773737  0.26233108
[36,]  0.49118828  0.94260085 -0.50508981 -0.25094662  0.79843377 -0.46788837  1.14712719 -0.31573248 -0.36032080  0.26270412
[37,] -0.32408658  0.82625829 -0.05215359  1.69934667  0.85086044  0.75837456  0.01479365 -0.57509610  0.35059377  0.64073888
[38,] -1.66205024 -0.81154049  0.62886063 -0.34429880 -0.44356797 -0.64173636 -0.31173924 -1.40635644  0.02825766  0.30708983
[39,]  1.76773385  0.47624828  2.18000240  0.06777206 -0.44677479  0.62767182 -0.95619611  2.26785985  0.47304826 -0.03312940
[40,]  0.02580105  1.02125841 -0.06901731 -0.65056973  0.01330504  0.24833013  0.47341376 -0.77085393 -0.91915473 -1.37475053

| You're close...I can feel it! Try it again. Or, type info() for more options.

| Type head(dataMatrix) at the command prompt.

> head(data.matrix())
Error in is.data.frame(frame) : 
  argument "frame" is missing, with no default
> head(dataMatrix
+ 

> head(dataMatrix)
           [,1]       [,2]       [,3]       [,4]       [,5]       [,6]       [,7]       [,8]       [,9]       [,10]
[1,]  0.5855288  1.1285108  0.6453831  1.5448636 -0.4876385 -1.4361457 -0.7000758 -1.5138641  0.3803157 -0.37582344
[2,]  0.7094660 -2.3803581  1.0431436  1.3214520  0.3031512 -0.6292596 -0.5674016  0.1642810  0.6051368 -1.81283376
[3,] -0.1093033 -1.0602656 -0.3043691  0.3221516 -0.2419740  0.2435218 -0.2613939 -0.8708652  1.0196741  0.28860021
[4,] -0.4534972  0.9371405  2.4771109  1.5309551 -0.4817336  1.0583622 -1.0638850  1.5933290  0.4749430 -0.18962258
[5,]  0.6058875  0.8544517  0.9712207 -0.4212397 -0.9918029  0.8313488 -0.1063687  0.6465975 -2.1859464  0.01786021
[6,] -1.8179560  1.4607294  1.8670992 -1.1588210 -0.2806491  0.1052118  0.7711037  0.3573697  0.9331922  0.65043024

| That's a job well done!

  |=======                                                                                                                                      |   5%

| So we see that dataMatrix has 10 columns (and hence 40 rows) of random numbers. The image here looks pretty random. Let's see how the data clusters.
| Run the R command heatmap with dataMatrix as its only argument.

> heatmap(dataMatrix)

| Great job!

  |========                                                                                                                                     |   6%

| We can see that even with the clustering that heatmap provides, permuting the rows (observations) and columns (variables) independently, the data
| still looks random.

...

  |==========                                                                                                                                   |   7%

| Let's add a pattern to the data. We've put some R code in the file addPatt.R for you. Run the command myedit with the single argument "addPatt.R"
| (make sure to use the quotation marks) to see the code. You might have to click your cursor in the console after you do this to keep from
| accidentally changing the file.

> 
> myedit("addPatt.R")

| You got it!

  |============                                                                                                                                 |   8%

| Look at the code. Will every row of the matrix have a pattern added to it?

1: No
2: Yes

Selection: 1

| You are doing so well!

  |==============                                                                                                                               |  10%

| So whether or not a row gets modified by a pattern is determined by a coin flip. Will the added pattern affect every column in the affected row?

1: Yes
2: No

Selection: 1

| Not quite! Try again.

| The expression rep(c(0,3),each=5) creates the 10-long vector (0,0,0,0,0,3,3,3,3,3) which is added to the rows chosen by the coin flip.

1: No
2: Yes

Selection: 2

| Nice try, but that's not exactly what I was hoping for. Try again.

| The expression rep(c(0,3),each=5) creates the 10-long vector (0,0,0,0,0,3,3,3,3,3) which is added to the rows chosen by the coin flip.

1: No
2: Yes

Selection: 1

| You are really on a roll!

  |===============                                                                                                                              |  11%

| So in rows affected by the coin flip, the 5 left columns will still have a mean of 0 but the right 5 columns will have a mean closer to 3.

...

  |=================                                                                                                                            |  12%

| Now to execute this code, run the R command source with 2 arguments. The first is the filename (in quotes), "addPatt.R", and the second is the
| argument local set equal to TRUE.

> source("addPatt.R",local = TRUE)

| You're the best!

  |===================                                                                                                                          |  13%

| Here's the image of the altered dataMatrix after the pattern has been added. The pattern is clearly visible in the columns of the matrix. The right
| half is yellower or hotter, indicating higher values in the matrix.

...

  |====================                                                                                                                         |  14%

| Now run the R command heatmap again with dataMatrix as its only argument. This will perform a hierarchical cluster analysis on the matrix.

> heatmap(dataMatrix)

| Excellent job!

  |======================                                                                                                                       |  16%

| Again we see the pattern in the columns of the matrix. As shown in the dendrogram at the top of the display, these split into 2 clusters, the lower
| numbered columns (1 through 5) and the higher numbered ones (6 through 10). Recall from the code in addPatt.R that for rows selected by the coinflip
| the last 5 columns had 3 added to them. The rows still look random.

...

  |========================                                                                                                                     |  17%

| Now consider this picture. On the left is an image similar to the heatmap of dataMatix you just plotted. It is an image plot of the output of
| hclust(), a hierarchical clustering function applied to dataMatrix. Yellow indicates "hotter" or higher values than red. This is consistent with the
| pattern we applied to the data (increasing the values for some of the rightmost columns).

...

  |=========================                                                                                                                    |  18%

| The middle display shows the mean of each of the 40 rows (along the x-axis). The rows are shown in the same order as the rows of the heat matrix on
| the left. The rightmost display shows the mean of each of the 10 columns. Here the column numbers are along the x-axis and their means along the y.

...

  |===========================                                                                                                                  |  19%

| We see immediately the connection between the yellow (hotter) portion of the cluster image and the higher row means, both in the upper right portion
| of the displays. Similarly, the higher valued column means are in the right half of that display and lower colummn means are in the left half.

...

  |=============================                                                                                                                |  20%

| Now we'll talk a little theory. Suppose you have 1000's of multivariate variables X_1, ... ,X_n. By multivariate we mean that each X_i contains many
| components, i.e., X_i = (X_{i1}, ... , X_{im}. However, these variables (observations) and their components might be correlated to one another.

...

  |===============================                                                                                                              |  22%

| Which of the following would be an example of variables correlated to one another?

1: Heights and weights of members of a family
2: The depth of the Atlantic Ocean and what you eat for breakfast
3: Today's weather and a butterfly's wing position

Selection: 1

| You are amazing!

  |================================                                                                                                             |  23%

| As data scientists, we'd like to find a smaller set of multivariate variables that are uncorrelated AND explain as much variance (or variability) of
| the data as possible. This is a statistical approach.

...

  |==================================                                                                                                           |  24%

| In other words, we'd like to find the best matrix created with fewer variables (that is, a lower rank matrix) that explains the original data. This
| is related to data compression.

...

  |====================================                                                                                                         |  25%

| Two related solutions to these problems are PCA which stands for Principal Component Analysis and SVD, Singular Value Decomposition. This latter
| simply means that we express a matrix X of observations (rows) and variables (columns) as the product of 3 other matrices, i.e., X=UDV^t. This last
| term (V^t) represents the transpose of the matrix V.

...

  |=====================================                                                                                                        |  27%

| Here U and V each have orthogonal (uncorrelated) columns. U's columns are the left singular vectors of X and V's columns are the right singular
| vectors of X.  D is a diagonal matrix, by which we mean that all of its entries not on the diagonal are 0. The diagonal entries of D are the
| singular values of X.

...

  |=======================================                                                                                                      |  28%

| To illustrate this idea we created a simple example matrix called mat. Look at it now.

> mat
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    2    5    7

| You are really on a roll!

  |=========================================                                                                                                    |  29%

| So mat is a 2 by 3 matrix. Lucky for us R provides a function to perform singular value decomposition. It's called, unsurprisingly, svd. Call it now
| with a single argument, mat.

> svd(mat)
$d
[1] 9.5899624 0.1806108

$u
           [,1]       [,2]
[1,] -0.3897782 -0.9209087
[2,] -0.9209087  0.3897782

$v
           [,1]       [,2]
[1,] -0.2327012 -0.7826345
[2,] -0.5614308  0.5928424
[3,] -0.7941320 -0.1897921


| You got it!

  |==========================================                                                                                                   |  30%

| We see that the function returns 3 components, d which holds 2 diagonal elements, u, a 2 by 2 matrix, and v, a 3 by 2 matrix. We stored the diagonal
| entries in a diagonal matrix for you, diag, and we also stored u and v in the variables matu and matv respectively. Multiply matu by diag by t(matv)
| to see what you get. (This last expression represents the transpose of matv in R). Recall that in R matrix multiplication requires you to use the
| operator %*%.

> matu%*%t(matv)
            [,1]       [,2]      [,3]
[1,]  0.81143686 -0.3271203 0.4843166
[2,] -0.09075735  0.7481036 0.6573463

| Keep trying! Or, type info() for more options.

| Type matu %*% diag %*% t(matv) at the command prompt.

> matu%*%diag%*%t(matv)
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    2    5    7

| You're the best!

  |============================================                                                                                                 |  31%

| So we did in fact get mat back. That's a relief! Note that this type of decomposition is NOT unique.

...

  |==============================================                                                                                               |  33%

| Now we'll talk a little about PCA, Principal Component Analysis, "a simple, non-parametric method for extracting relevant information from confusing
| data sets." We're quoting here from a very nice concise paper on this subject which can be found at http://arxiv.org/pdf/1404.1100.pdf. The paper by
| Jonathon Shlens of Google Research is called, A Tutorial on Principal Component Analysis.

...

  |================================================                                                                                             |  34%

| Basically, PCA is a method to reduce a high-dimensional data set to its essential elements (not lose information) and explain the variability in the
| data. We won't go into the mathematical details here, (R has a function to perform PCA), but you should know that SVD and PCA are closely related.

...

  |=================================================                                                                                            |  35%

| We'll demonstrate this now. First we have to scale mat, our simple example data matrix.  This means that we subtract the column mean from every
| element and divide the result by the column standard deviation. Of course R has a command, scale, that does this for you. Run svd on scale of mat.

> svd(scale(mat))
$d
[1] 1.732051 0.000000

$u
           [,1]      [,2]
[1,] -0.7071068 0.7071068
[2,]  0.7071068 0.7071068

$v
          [,1]       [,2]
[1,] 0.5773503 -0.5773503
[2,] 0.5773503  0.7886751
[3,] 0.5773503 -0.2113249


| You are quite good my friend!

  |===================================================                                                                                          |  36%

| Now run the R program prcomp on scale(mat). This will give you the principal components of mat. See if they look familiar.

> prcomp(scale(mat))
Standard deviations (1, .., p=2):
[1] 1.732051 0.000000

Rotation (n x k) = (3 x 2):
           PC1        PC2
[1,] 0.5773503 -0.5773503
[2,] 0.5773503  0.7886751
[3,] 0.5773503 -0.2113249

| You are quite good my friend!

  |=====================================================                                                                                        |  37%

| Notice that the principal components of the scaled matrix, shown in the Rotation component of the prcomp output, ARE the columns of V, the right
| singular values. Thus, PCA of a scaled matrix yields the V matrix (right singular vectors) of the same scaled matrix.

...

  |======================================================                                                                                       |  39%

| Now that we covered the theory let's return to our bigger matrix of random data into which we had added a fixed pattern for some rows selected by
| coinflips. The pattern effectively shifted the means of the rows and columns.

...

  |========================================================                                                                                     |  40%

| Here's a picture showing the relationship between PCA and SVD for that bigger matrix.  We've plotted 10 points (5 are squished together in the
| bottom left corner). The x-coordinates are the elements of the first principal component (output from prcomp), and the y-coordinates are the
| elements of the first column of V, the first right singular vector (gotten from running svd). We see that the points all lie on the 45 degree line
| represented by the equation y=x.  So the first column of V IS the first principal component of our bigger data matrix.

...

  |==========================================================                                                                                   |  41%

| To prove we're not making this up, we've run svd on dataMatrix and stored the result in the object svd1. This has 3 components, d, u and v. look at
| the first column of V now. It can be viewed by using the svd1$v[,1] notation.

> svd1
$d
 [1] 12.458121  7.779798  6.732595  6.301878  5.860013  4.501826  3.921267  2.973909  2.401470  2.152848

$u
             [,1]         [,2]         [,3]          [,4]         [,5]         [,6]        [,7]         [,8]          [,9]        [,10]
 [1,] -0.14105455  0.056838658  0.029063223  0.2752378645 -0.188921289 -0.244534189  0.11319947  0.203457098 -0.0670929147  0.213254663
 [2,] -0.13890797  0.108042918  0.273689886  0.2834695760 -0.140556349  0.138516256  0.07779046  0.025955855  0.0591451899  0.014593112
 [3,] -0.15326905  0.317209414 -0.176355518 -0.1357006024 -0.115124985  0.308107364  0.03418977  0.300415382  0.2735951823  0.049072243
 [4,] -0.12875622  0.191455334 -0.184964526  0.1473785723 -0.062761958 -0.137142601  0.15455272  0.009696959 -0.0819476426 -0.080990382
 [5,] -0.08905957  0.199647582 -0.254214538  0.2170911231  0.042897710  0.086453758 -0.03352080 -0.004785945  0.0343634109  0.067490734
 [6,] -0.09789417 -0.327861576 -0.117467359 -0.1275543366  0.160950663 -0.210726235  0.12893909  0.338594240  0.0446357986 -0.224580135
 [7,] -0.01126703  0.127128916  0.127333286  0.0859243684  0.196392022  0.002587659 -0.23910014 -0.012609144 -0.0872305967 -0.030863478
 [8,] -0.04883403 -0.064041570 -0.049493184  0.1835712767  0.221630414  0.186731758 -0.31646974 -0.038645506  0.1230494545  0.061502180
 [9,] -0.07287826  0.227334699  0.047848612  0.0200375229 -0.140444025 -0.380462933  0.11681916 -0.210188586  0.1427937405 -0.052630974
[10,] -0.02907261 -0.002181485  0.319330475 -0.0189601051 -0.006185793 -0.169417221 -0.21791161 -0.282631259  0.0603774926 -0.144968996
[11,] -0.16970155 -0.302289844  0.009561794 -0.0298244876 -0.125536038 -0.030430830  0.04428341  0.127048597  0.0643655754  0.126911112
[12,] -0.12607341 -0.275138514 -0.069267213 -0.2021618336  0.138303934  0.197779355  0.06736826 -0.366132896  0.1022529534  0.286891270
[13,] -0.20210126 -0.151074580  0.059594365  0.0998823538  0.141666889  0.002403037  0.08533918 -0.226957186  0.0384140856  0.209393755
[14,] -0.19378533 -0.050002134  0.033035549 -0.2748551468  0.070706139 -0.189181366  0.12108909  0.125756516 -0.2898651803  0.116332057
[15,] -0.11823805  0.070248644  0.215625347  0.0448021108 -0.037510888 -0.114542995 -0.14638318  0.192464758 -0.0616925821 -0.022537731
[16,] -0.12021471  0.137239914 -0.069911268 -0.1195663566  0.040766731 -0.025719851 -0.01020849  0.006760584 -0.0026171706 -0.168508062
[17,] -0.11304998  0.002399633  0.082164842 -0.1381848285  0.025207925  0.177377248 -0.13439168  0.022029018 -0.0601215643 -0.073416312
[18,] -0.12292726 -0.105737723 -0.079606207  0.0593038593  0.098237441 -0.035214944 -0.14344373  0.003745017 -0.2478903695  0.186535853
[19,] -0.12408459  0.009362813  0.065739602 -0.0165667373  0.323174865 -0.181872635  0.02825647  0.044218443 -0.0744535299  0.051805975
[20,] -0.16862945 -0.102660603 -0.063557327  0.0420115408  0.036797117  0.155718451  0.25760955 -0.009854514  0.0968205056 -0.390259692
[21,] -0.12165026 -0.096171108 -0.241136337  0.0244309478 -0.006242241  0.169188426 -0.01130238  0.055477357  0.0351431451 -0.280987453
[22,] -0.17502622  0.078086247  0.020283154  0.0198608347 -0.116874263  0.274369047 -0.11758721 -0.209968271 -0.3305963697 -0.227622347
[23,] -0.16546733  0.018367782  0.057252037 -0.2260790152 -0.219725521 -0.014230288 -0.03100011 -0.063514364 -0.0013925430  0.052761674
[24,] -0.13656414  0.162710793  0.039049142 -0.0910655769 -0.094300948  0.041768815  0.12765054 -0.160204249  0.1551574135  0.199600963
[25,]  0.21500524  0.023391506  0.080900105  0.0660488485 -0.124988397  0.208948895  0.13147799  0.209301114 -0.2980002351  0.178870622
[26,]  0.26902554  0.039843638 -0.074794592  0.0680204261  0.089001052 -0.001298850  0.38903372 -0.001022715 -0.2957744180  0.060423727
[27,]  0.18307788  0.097518635  0.389455810 -0.0009410456  0.032870003 -0.033299560  0.17350323 -0.028439011  0.1516082354  0.040534065
[28,]  0.23164192 -0.155910366  0.151467122  0.3160982994 -0.080798291  0.236740848  0.20101447 -0.041381515  0.1969551934 -0.052167673
[29,]  0.17210362  0.124182968 -0.126520526 -0.0820850791  0.197747241 -0.168140907  0.18799405 -0.252795745  0.1623605152 -0.232155433
[30,]  0.17174938  0.159337178 -0.020035926 -0.0982853744  0.188457198  0.153911454 -0.05433623  0.167503288 -0.0601914477 -0.012344470
[31,]  0.22996415  0.340588676 -0.092580405 -0.2061365896  0.252150623 -0.008752518 -0.06673194 -0.027703937  0.0243100468  0.227491191
[32,]  0.15284317 -0.133146996 -0.170180797  0.2896125456  0.254311184 -0.159200486 -0.24074089  0.022047156  0.0594267744 -0.045719124
[33,]  0.12981249 -0.041041799  0.180191812  0.1419493886  0.103698411 -0.069123352 -0.16162205  0.271791960  0.1660537728 -0.027974367
[34,]  0.11361322 -0.089197916 -0.146216019 -0.0402006348 -0.404328837 -0.222531237 -0.29374294  0.008016211  0.0007415506  0.009661833
[35,]  0.18650320  0.015537054 -0.362486286  0.0541198397 -0.276096088 -0.039188197 -0.19632963 -0.130645313  0.0564229319  0.149647415
[36,]  0.14881901 -0.292607329 -0.104906133 -0.0584473704 -0.057661883 -0.006271998  0.12356719  0.041484937  0.2506460873  0.236938847
[37,]  0.16285835 -0.099569076 -0.015117752  0.0675708406 -0.187652824 -0.049610157  0.01263948 -0.145048675 -0.1154906146 -0.236711416
[38,]  0.23376268 -0.102629841 -0.045978669 -0.0391203500 -0.043503356  0.044085545  0.03247730 -0.145684720 -0.3711122015 -0.059100786
[39,]  0.14362847 -0.145059852  0.269018604 -0.2579859750 -0.090595906  0.146521956 -0.15527446  0.059546846 -0.0113885387 -0.032332931
[40,]  0.22409869  0.029849311  0.014185816 -0.3427006942 -0.095157684 -0.040316525 -0.03869736  0.122902213  0.1582188634 -0.143841533

$v
             [,1]        [,2]        [,3]        [,4]       [,5]         [,6]        [,7]         [,8]        [,9]       [,10]
 [1,] -0.01269600  0.17735388 -0.24463274  0.91004953 -0.1872010  0.008213019 -0.06456695  0.024609115 -0.07464230 -0.18679254
 [2,]  0.11959541 -0.48797855  0.34600069  0.27630307  0.5166116  0.426097876 -0.21673650 -0.089149131 -0.20830183  0.06895739
 [3,]  0.03336723  0.01498233  0.83325514  0.10279613 -0.3916654 -0.202164641 -0.08613180  0.259091961  0.04853894 -0.15035070
 [4,]  0.09405542  0.52706147  0.20958117  0.13573720  0.6295808 -0.457199442  0.04669870 -0.044052013  0.01123676  0.20337161
 [5,] -0.12201820 -0.65260072 -0.10178071  0.14269264  0.1099219 -0.641254633  0.17040991 -0.053081113  0.25570702 -0.08927377
 [6,] -0.43175437  0.03340040  0.10662102 -0.01977183  0.1601515  0.094552382  0.65292409  0.120799342 -0.43700714 -0.36639783
 [7,] -0.44120227 -0.05458792 -0.06283533 -0.06024994 -0.1160080 -0.297932642 -0.46970212  0.003486847 -0.62595889  0.28302631
 [8,] -0.43732624  0.01052127  0.18790005  0.18937336 -0.1696210  0.183059088  0.30154820 -0.318224392  0.27506907  0.63821893
 [9,] -0.44207248  0.14804603  0.10705233 -0.07740214  0.1113648  0.051388711 -0.35693677 -0.527133486  0.28235733 -0.51493770
[10,] -0.43924243  0.01981965 -0.09878099  0.01546923  0.2364994  0.152997807 -0.21201954  0.725133698  0.38076511  0.03978411


| Not quite, but you're learning! Try again. Or, type info() for more options.

| Type svd1$v[,1] at the command prompt.

> svd1$v[,1]
 [1] -0.01269600  0.11959541  0.03336723  0.09405542 -0.12201820 -0.43175437 -0.44120227 -0.43732624 -0.44207248 -0.43924243

| That's correct!

  |===========================================================                                                                                  |  42%

| See how these values correspond to those plotted? Five of the entries are slightly to the left of the point (-0.4,-0.4), two more are negative (to
| the left of (0,0)), and three are positive (to the right of (0,0)).

...

  |=============================================================                                                                                |  43%

| Here we again show the clustered data matrix on the left. Next to it we've plotted the first column of the U matrix associated with the scaled data
| matrix. This is the first LEFT singular vector and it's associated with the ROW means of the clustered data. You can see the clear separation
| between the top 24 (around -0.2) row means and the bottom 16 (around 0.2). We don't show them but note that the other columns of U don't show this
| pattern so clearly.

...

  |===============================================================                                                                              |  45%

| The rightmost display shows the first column of the V matrix associated with the scaled and clustered data matrix. This is the first RIGHT singular
| vector and it's associated with the COLUMN means of the clustered data. You can see the clear separation between the left 5 column means (between
| -0.1 and 0.1) and the right 5 column means (all below -0.4). As with the left singular vectors, the other columns of V don't show this pattern as
| clearly as this first one does.

...

  |=================================================================                                                                            |  46%

| So the singular value decomposition automatically picked up these patterns, the differences in the row and column means.

...

  |==================================================================                                                                           |  47%

| Why were the first columns of both the U and V matrices so special?  Well as it happens, the D matrix of the SVD explains this phenomenon. It is an
| aspect of SVD called variance explained. Recall that D is the diagonal matrix sandwiched in between U and V^t in the SVD representation of the data
| matrix. The diagonal entries of D are like weights for the U and V columns accounting for the variation in the data. They're given in decreasing
| order from highest to lowest. Look at these diagonal entries now. Recall that they're stored in svd1$d.

> svd1$d
 [1] 12.458121  7.779798  6.732595  6.301878  5.860013  4.501826  3.921267  2.973909  2.401470  2.152848

| You are amazing!

  |====================================================================                                                                         |  48%

| Here's a display of these values (on the left). The first one (12.46) is significantly bigger than the others. Since we don't have any units
| specified, to the right we've plotted the proportion of the variance each entry represents. We see that the first entry accounts for about 40% of
| the variance in the data. This explains why the first columns of the U and V matrices respectively showed the distinctive patterns in the row and
| column means so clearly.

...

  |======================================================================                                                                       |  49%

| Now we'll show you another simple example of how SVD explains variance. We've created a 40 by 10 matrix, constantMatrix. Use the R command head with
| constantMatrix as its argument to see the top rows.

> head(constantMatrix)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    0    0    0    0    0    1    1    1    1     1
[2,]    0    0    0    0    0    1    1    1    1     1
[3,]    0    0    0    0    0    1    1    1    1     1
[4,]    0    0    0    0    0    1    1    1    1     1
[5,]    0    0    0    0    0    1    1    1    1     1
[6,]    0    0    0    0    0    1    1    1    1     1

| You nailed it! Good job!

  |=======================================================================                                                                      |  51%

| The rest of the rows look just like these. You can see that the left 5 columns are all 0's and the right 5 columns are all 1's. We've run svd with
| constantMatrix as its argument for you and stored the result in svd2. Look at the diagonal component, d, of svd2 now.

> svd2$d
 [1] 1.414214e+01 1.293147e-15 2.515225e-16 8.585184e-31 9.549693e-32 3.330034e-32 2.022600e-46 4.362170e-47 1.531252e-61 0.000000e+00

| Perseverance, that's the answer.

  |=========================================================================                                                                    |  52%

| Which index holds the largest entry of the svd2$d?

1: 9
2: 5
3: 1
4: 10

Selection: 3

| You are doing so well!

  |===========================================================================                                                                  |  53%

| So the first entry by far dominates the others. Here the picture on the left shows the heat map of constantMatrix. You can see how the left columns
| differ from the right ones. The middle plot shows the values of the singular values of the matrix, i.e., the diagonal elements which are the entries
| of svd2$d. Nine of these are 0 and the first is a little above 14. The third plot shows the proportion of the total each diagonal element
| represents.

...

  |============================================================================                                                                 |  54%

| According to the plot, what percentage of the total variation does the first diagonal element account for?

1: 0%
2: 100%
3: 90%
4: 50%

Selection: 2

| You nailed it! Good job!

  |==============================================================================                                                               |  55%

| So what does this mean? Basically that the data is one-dimensional. Only 1 piece of information, namely which column an entry is in, determines its
| value.

...

  |================================================================================                                                             |  57%

| Now let's return to our random 40 by 10 dataMatrix and consider a slightly more complicated example in which we add 2 patterns to it. Again we'll
| choose which rows to tweak using coinflips. Specifically, for each of the 40 rows we'll flip 2 coins. If the first coinflip is heads, we'll add 5 to
| each entry in the right 5 columns of that row, and if the second coinflip is heads, we'll add 5 to just the even columns of that row.

...

  |==================================================================================                                                           |  58%

| So here's the image of the scaled data matrix on the left. We can see both patterns, the clear difference between the left 5 and right 5 columns,
| but also, slightly less visible, the alternating pattern of the columns. The other plots show the true patterns that were added into the affected
| rows. The middle plot shows the true difference between the left and right columns, while the rightmost plot shows the true difference between the
| odd numbered and even-numbered columns.

...

  |===================================================================================                                                          |  59%

| The question is, "Can our analysis detect these patterns just from the data?" Let's see what SVD shows. Since we're interested in patterns on
| columns we'll look at the first two right singular vectors (columns of V) to see if they show any evidence of the patterns.

...

  |=====================================================================================                                                        |  60%

| Here we see the 2 right singular vectors plotted next to the image of the data matrix. The middle plot shows the first column of V and the rightmost
| plot the second. The middle plot does show that the last 5 columns have higher entries than the first 5. This picks up, or at least alludes to, the
| first pattern we added in which affected the last 5 columns of the matrix. The rightmost plot, showing the second column of V, looks more random.
| However, closer inspection shows that the entries alternate or bounce up and down as you move from left to right. This hints at the second pattern
| we added in which affected only even columns of selected rows.

...

  |=======================================================================================                                                      |  61%

| To see this more closely, look at the first 2 columns of the v component. We stored the SVD output in the svd object svd2.

> svd2$v[,2]
 [1]  0.142468636  0.504510087  0.316470664  0.524499356 -0.282921362 -0.002280468 -0.354403893  0.039226153 -0.376485206 -0.031422705

| Keep trying! Or, type info() for more options.

| Type svd2$v[,1:2] at the command prompt.

> svd2$v[,1:2]
            [,1]         [,2]
 [1,] 0.06154540  0.142468636
 [2,] 0.26433096  0.504510087
 [3,] 0.04987554  0.316470664
 [4,] 0.27693897  0.524499356
 [5,] 0.14275820 -0.282921362
 [6,] 0.43252652 -0.002280468
 [7,] 0.37724057 -0.354403893
 [8,] 0.43280767  0.039226153
 [9,] 0.34912246 -0.376485206
[10,] 0.43379723 -0.031422705

| Great job!

  |========================================================================================                                                     |  63%

| Seeing the 2 columns side by side, we see that the values in both columns alternately increase and decrease. However, we knew to look for this
| pattern, so chances are, you might not have noticed this pattern if you hadn't known if was there. This example is meant to show you that it's hard
| to see patterns, even straightforward ones.

...

  |==========================================================================================                                                   |  64%

| Now look at the entries of the diagonal matrix d resulting from the svd. Recall that we stored this output for you in the svd object svd2.

> svd2$d
 [1] 14.084918  8.257842  6.458184  6.100500  5.538262  2.266358  1.955790  1.609508  1.078566  1.054146

| You got it right!

  |============================================================================================                                                 |  65%

| We see that the first element, 14.55, dominates the others. Here's the plot of these diagonal elements of d. The left shows the numerical entries
| and the right show the percentage of variance each entry explains.

...

  |=============================================================================================                                                |  66%

| According to the plot, how much of the variance does the second element account for?

1: 18%
2: 11%
3: 53%
4: .1%

Selection: 1

| That's a job well done!

  |===============================================================================================                                              |  67%

| So the first element which showed the difference between the left and right halves of the matrix accounts for roughly 50% of the variation in the
| matrix, and the second element which picked up the alternating pattern accounts for 18% of the variance. The remaining elements account for smaller
| percentages of the variation. This indicates that the first pattern is much stronger than the second. Also the two patterns confound each other so
| they're harder to separate and see clearly. This is what often happens with real data.

...

  |=================================================================================================                                            |  69%

| Now you're probably convinced that SVD and PCA are pretty cool and useful as tools for analysis, but one problem with them that you should be aware
| of, is that they cannot deal with MISSING data. Neither of them will work if any data in the matrix is missing. (You'll get error messages from R in
| red if you try.) Missing data is not unusual, so luckily we have ways to work around this problem. One we'll just mention is called imputing the
| data.

...

  |===================================================================================================                                          |  70%

| This uses the k nearest neighbors to calculate a values to use in place of the missing data. You may want to specify an integer k which indicates
| how many neighbors you want to average to create this replacement value. The bioconductor package (http://bioconductor.org) has an impute package
| which you can use to fill in missing data. One specific function in it is impute.knn.

...

  |====================================================================================================                                         |  71%

| We'll move on now to a final example of the power of singular value decomposition and principal component analysis and how they work as a data
| compression technique.

...

  |======================================================================================================                                       |  72%

| Consider this low resolution image file showing a face. We'll use SVD and see how the first several components contain most of the information in
| the file so that storing a huge matrix might not be necessary.

...

  |========================================================================================================                                     |  73%

| The image data is stored in the matrix faceData. Run the R command dim on faceData to see how big it is.

> dim(faceData)
[1] 32 32

| You got it right!

  |=========================================================================================================                                    |  75%

| So it's not that big of a file but we want to show you how to use what you learned in this lesson. We've done the SVD and stored it in the object
| svd1 for you. Here's the plot of the variance explained.

...

  |===========================================================================================================                                  |  76%

| According to the plot what percentage of the variance is explained by the first singular value?

1: 15
2: 100
3: 40
4: 23

Selection: 3

| Excellent work!

  |=============================================================================================================                                |  77%

| So 40% of the variation in the data matrix is explained by the first component, 22% by the second, and so forth. It looks like most of the variation
| is contained in the first 10 components. How can we check this out? Can we try to create an approximate image using only a few components?

...

  |==============================================================================================================                               |  78%

| Recall that the data matrix X is the product of 3 matrices, that is X=UDV^t. These are precisely what you get when you run svd on the matrix X.

...

  |================================================================================================================                             |  80%

| Suppose we create the product of pieces of these, say the first columns of U and V and the first element of D. The first column of U can be
| interpreted as a 32 by 1 matrix (recall that faceData was a 32 by 32 matrix), so we can multiply it by the first element of D, a 1 by 1 matrix, and
| get a 32 by 1 matrix result. We can multiply that by the transpose of the first column of V, which is the first principal component. (We have to use
| the transpose of V's column to make it a 1 by 32 matrix in order to do the matrix multiplication properly.)

...

  |==================================================================================================================                           |  81%

| Alas, that is how we do it in theory, but in R using only one element of d means it's a constant. So we have to do the matrix multiplication with
| the %*% operator and the multiplication by the constant (svd1$d[1]) with the regular multiplication operator *.

...

  |====================================================================================================================                         |  82%

| Try this now and put the result in the variable a1. Recall that svd1$u, svd1$d, and svd1$v contain all the information you need. NOTE that because
| of the peculiarities of R's casting, if you do the scalar multiplication with the * operator first (before the matrix multiplication with the %*%
| operator) you MUST enclose the 2 arguments (svd1$u[,1] and svd1$d[1]) in parentheses.

> a1<-(svd1$d)*(svd1$u%*%svd1$v)

| Not quite! Try again. Or, type info() for more options.

| Type a1 <- (svd1$u[,1] * svd1$d[1]) %*% t(svd1$v[,1]) OR a1 <- svd1$u[,1] %*% t(svd1$v[,1]) * svd1$d[1] at the command prompt.

> a1<-(svd1$d[,1])*(svd1$u%*%t(svd1$v))
Error in svd1$d[, 1] : incorrect number of dimensions
> a1<-(svd1$d[,1])*(svd1$u[,1]%*%t(svd1$v[,1]))
Error in svd1$d[, 1] : incorrect number of dimensions
> a1<-svd1$d[,1]*(svd1$u[,1]%*%t(svd1$v[,1]))
Error in svd1$d[, 1] : incorrect number of dimensions
> a1<-(svd1$u[,1]*svd1$d[,1])%*%t(svd1$v[,1]))
Error: unexpected ')' in "a1<-(svd1$u[,1]*svd1$d[,1])%*%t(svd1$v[,1]))"
> a1<-(svd1$u[,1]*svd1$d[,1])%*%t(svd1$v[,1])
Error in svd1$d[, 1] : incorrect number of dimensions
> a1<-(svd1$u[,1]*svd1$d[ ,1])%*%t(svd1$v[,1])
Error in svd1$d[, 1] : incorrect number of dimensions
> a1 <- (svd1$u[,1] * svd1$d[1]) %*% t(svd1$v[,1])

| You are amazing!

  |=====================================================================================================================                        |  83%

| Now to look at it as an image. We wrote a function for you called myImage which takes a single argument, a matrix of data to display using the R
| function image. Run it now with a1 as its argument.

> myImage(a1)

| Excellent job!

  |=======================================================================================================================                      |  84%

| It might not look like much but it's a good start. Now we'll try the same experiment but this time we'll use 2 elements from each of the 3 SVD
| terms.

...

  |=========================================================================================================================                    |  86%

| Create the matrix a2 as the product of the first 2 columns of svd1$u, a diagonal matrix using the first 2 elements of svd1$d, and the transpose of
| the first 2 columns of svd1$v. Since all of your multiplicands are matrices you have to use only the operator %*% AND you DON'T need parentheses.
| Also, you must use the R function diag with svd1$d[1:2] as its sole argument to create the proper diagonal matrix. Remember, matrix multiplication
| is NOT commutative so you have to put the multiplicands in the correct order. Please use the 1:2 notation and not the c(m:n), i.e., the concatenate
| function, when specifying the columns.

> a2<-svd1$u[,1:2]%*%diag(svd1$d[,1:2])%*%t(svd1$v[,1:2])
Error in svd1$d[, 1:2] : incorrect number of dimensions
> a2<-svd1$u[,1:2]%*%diag(svd1$d[1:2])%*%t(svd1$v[,1:2])

| You are amazing!

  |==========================================================================================================================                   |  87%

| Use myImage again to see how a2 displays.

> myImage(a2)

| All that practice is paying off!

  |============================================================================================================================                 |  88%

| We're starting to see slightly more detail, and maybe if you squint you see a grimacing mouth. Now let's see what image results using 5 components.
| From our plot of the variance explained 5 components covered a sizeable percentage of the variation. To save typing, use the up arrow to recall the
| command which created a2 and replace the a2 and assignment arrow with the call to myImage, and change the three occurrences of 2 to 5.

> myimage(svd1$u[,1:5]%*%diag(svd1$d[1:5])%*%t(svd1$v[,1:5]))
Error in myimage(svd1$u[, 1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[, 1:5])) : 
  could not find function "myimage"
> myImage(svd1$u[,1:5]%*%diag(svd1$d[1:5])%*%t(svd1$v[,1:5]))

| That's a job well done!

  |==============================================================================================================================               |  89%

| Certainly much better. Clearly a face is appearing with eyes, nose, ears, and mouth recognizable. Again, use the up arrow to recall the last command
| (calling myImage with a matrix product argument) and change the 5's to 10's. We'll see how this image looks.

> myImage(svd1$u[,1:10]%*%diag(svd1$d[1:10])%*%t(svd1$v[,1:10]))

| You are quite good my friend!

  |===============================================================================================================================              |  90%

| Now that's pretty close to the original which was low resolution to begin with, but you can see that 10 components really do capture the essence of
| the image. Singular value decomposition is a good way to approximate data without having to store a lot.

...

  |=================================================================================================================================            |  92%

| We'll close now with a few comments. First, when reducing dimensions you have to pay attention to the scales on which different variables are
| measured and make sure that all your data is in consistent units. In other words, scales of your data matter. Second, principal components and
| singular values may mix real patterns, as we saw in our simple 2-pattern example, so finding and separating out the real patterns require some
| detective work. Let's do a quick review now.

...

  |===================================================================================================================================          |  93%

| Which of the following cliches LEAST captures the essence of dimension reduction?

1: find the needle in the haystack
2: see the forest through the trees
3: a face that could launch a 1000 ships
4: separate the wheat from the chaff

Selection: 1

| One more time. You can do it!

| Which choice fails to deal with discerning differences between the valuable and the invaluable.

1: see the forest through the trees
2: separate the wheat from the chaff
3: a face that could launch a 1000 ships
4: find the needle in the haystack

Selection: 3

| You nailed it! Good job!

  |=====================================================================================================================================        |  94%

| A matrix X has the singular value decomposition UDV^t. The principal components of X are ?

1: the rows of U
2: the columns of U
3: the rows of V
4: the columns of V

Selection: 2

| Try again. Getting it right on the first try is boring anyway!

| Recall the simple example where we ran prcomp and svd on the same scaled matrix and saw that the columns of V matched the rotations of the prcomp
| output.

1: the columns of U
2: the rows of V
3: the rows of U
4: the columns of V

Selection: 4

| You're the best!

  |======================================================================================================================================       |  95%

| A matrix X has the singular value decomposition UDV^t. The singular values of X are found where?

1: the columns of U
2: the diagonal elements of D
3: the columns of D
4: the columns of V

Selection: 2

| Excellent job!

  |========================================================================================================================================     |  96%

| True or False? PCA and SVD are totally unrelated.

1: False
2: True

Selection: 1

| You got it!

  |==========================================================================================================================================   |  98%

| True or False? D gives the singular values of a matrix in decreasing order of weight.

1: True
2: False
